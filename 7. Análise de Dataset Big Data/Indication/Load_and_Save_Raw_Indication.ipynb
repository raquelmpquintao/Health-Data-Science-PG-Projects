{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c489c941-7f00-471a-b4b1-471deb84cfde",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Notebook 1: Load_and_Save_Raw_Indication\n",
    "\n",
    "## Objetivo\n",
    "Carregar os dados de todos os 5 trimestres de `Indication` e salvar em formato Parquet.\n",
    "\n",
    "## Passos\n",
    "\n",
    "1. **Carregar dados de todos os trimestres**\n",
    "    - Vamos carregar os dados de `Indication` para os trimestres 2022Q4, 2023Q1, 2023Q2, 2023Q3 e 2023Q4.\n",
    "    \n",
    "2. **Criar DataFrames**\n",
    "    - Cada trimestre será carregado em um DataFrame separado.\n",
    "    \n",
    "3. **Concatenar DataFrames**\n",
    "    - Todos os DataFrames serão concatenados em um único DataFrame.\n",
    "    \n",
    "4. **Salvar em formato Parquet**\n",
    "    - O DataFrame concatenado será salvo em formato Parquet para uso eficiente em análises futuras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92e6f419-af3b-4a42-b640-10ad126d75ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trimestres = ['2022Q4', '2023q1', '2023q2', '2023Q3', '2023Q4']\n",
    "df_indication = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01a80b03-9b3e-4952-8cd3-cd072be68516",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo carregado com sucesso: /FileStore/faers-downloaded/faers_ascii_2022Q4/ASCII/INDI22Q4.txt\nArquivo carregado com sucesso: /FileStore/faers-downloaded/faers_ascii_2023q1/ASCII/INDI23Q1.txt\nArquivo carregado com sucesso: /FileStore/faers-downloaded/faers_ascii_2023q2/ASCII/INDI23Q2.txt\nArquivo carregado com sucesso: /FileStore/faers-downloaded/faers_ascii_2023Q3/ASCII/INDI23Q3.txt\nArquivo carregado com sucesso: /FileStore/faers-downloaded/faers_ascii_2023Q4/ASCII/INDI23Q4.txt\n"
     ]
    }
   ],
   "source": [
    "# Carregar e concatenar dados de todos os trimestres\n",
    "for trimestre in trimestres:\n",
    "    ano = trimestre[:4]\n",
    "    quarter = trimestre[-1].upper()  # Garantir que o quarter esteja em maiúsculas\n",
    "    trimestre_normalizado = trimestre[:-1] + quarter  # Normalizar o trimestre para que o quarter esteja em maiúsculas\n",
    "    path = f'/FileStore/faers-downloaded/faers_ascii_{trimestre_normalizado}/ASCII/INDI{ano[-2:]}Q{quarter}.txt'\n",
    "    try:\n",
    "        df = spark.read.csv(path, sep='$', header=True, inferSchema=True)\n",
    "        df_indication.append(df)\n",
    "        print(f\"Arquivo carregado com sucesso: {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar {path}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c47536ba-bc7b-46ce-a4d5-8d9f46642154",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Concatenar todos os DataFrames\n",
    "if df_indication:\n",
    "    df_indication_all = df_indication[0]\n",
    "    for df in df_indication[1:]:\n",
    "        df_indication_all = df_indication_all.union(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b6272e1-f078-4a33-8918-76a5bcd47239",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Definir o caminho completo incluindo o nome do arquivo Parquet\n",
    "output_path = 'dbfs:/FileStore/FAERS-grupo-4/indication_raw'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "034f17fd-a8d5-4209-8bb9-26f52f03613f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Função para gravar o DataFrame transformado em Parquet\n",
    "def salvar_parquet(df, path):\n",
    "    df.write.mode(\"overwrite\").parquet(path)\n",
    "    print(f\"Dados salvos em Parquet no caminho: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a18cf281-48e9-4e6c-8b8d-915f799f6850",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados de todos os quarters carregados e salvos em formato Parquet.\n"
     ]
    }
   ],
   "source": [
    "# Conclusão\n",
    "print(\"Dados de todos os quarters carregados e salvos em formato Parquet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93aa43b9-fa27-460a-9b83-52a6553a7f1b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1d2916e-11d8-4919-81c7-a800c19f671f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados salvos em Parquet no caminho: dbfs:/FileStore/FAERS-grupo-4/indication_raw\n"
     ]
    }
   ],
   "source": [
    "# Verificar se df_indication_all é um DataFrame antes de salvar\n",
    "if isinstance(df_indication_all, DataFrame):\n",
    "    salvar_parquet(df_indication_all, output_path)\n",
    "else:\n",
    "    print(\"Erro: df_indication_all não é um DataFrame\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Load_and_Save_Raw_Indication",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
