{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "290351a8-f45d-410f-a79c-ea3668e9efe7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Notebook 3: Funções de Transformação e Limpeza de Dados\n",
    "\n",
    "## Introdução\n",
    "Neste notebook, iremos implementar as transformações e limpar os dados da tabela 'Outcome' do conjunto de dados FAERS, com base na análise exploratória realizada no notebook anterior (Exploratory_Data_Analysis_Outcome). \n",
    "\n",
    "## Estrutura do notebook\n",
    "- Importação de Bibliotecas\n",
    "- Carregamento dos Dados (Parquet Raw) usando o Schema definido \n",
    "- Aplicar apenas as Transformações necessárias\n",
    "- Salvar **Parquet Final** com os Dados Limpos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2f90a46-ea2f-435e-9741-a2f0019cc3ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importar bibliotecas necessárias\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType, LongType\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "147d2cb6-4143-4203-835c-1d71f52e04e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../Funcoes_auxiliares\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2f35ec4-be5c-44f9-8958-142975c96f92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Carregar dados do Parquet raw\n",
    "schema_outc = StructType([\n",
    "    StructField(\"primaryid\", LongType(), False,{'description': \"Unique number for identifying a FAERS report.\"}),\n",
    "    StructField(\"caseid\", IntegerType(), False,{'description': \"Number for identifying a FAERS case.\"}),\n",
    "    StructField(\"outc_cod\", StringType(), False, {'description': \"Code for a patient outcome.\"})\n",
    "    ])\n",
    "df_outc = spark.read.format('parquet').schema(schema_outc).load('dbfs:/FileStore/FAERS-grupo-4/outc_raw')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94324001-4ee3-4038-9a52-96bdaa7ffbfe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Criar DataFrame de mapeamento de códigos para descrições\n",
    "mapping_data = [\n",
    "    (\"DE\", \"Death\"),\n",
    "    (\"LT\", \"Life Threatening\"),\n",
    "    (\"HO\", \"Hospitalization - Initial or Prolonged\"),\n",
    "    (\"DS\", \"Disability\"),\n",
    "    (\"CA\", \"Congenital Anomaly\"),\n",
    "    (\"RI\", \"Required Intervention to Prevent Permanent Impairment/Damage\"),\n",
    "    (\"OT\", \"Other Serious (Important Medical Event)\")\n",
    "]\n",
    "mapping_columns = [\"outc_cod\", \"outcome_name\"]\n",
    "df_mapping = spark.createDataFrame(mapping_data, mapping_columns)\n",
    "\n",
    "# Join entre df_outc e o DataFrame de mapeamento\n",
    "df_outc = df_outc.join(df_mapping, on=\"outc_cod\", how=\"left\")['primaryid','caseid','outc_cod','outcome_name']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df8eddde-6237-4e05-9366-a2ec64c37834",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Restaurar metadados\n",
    "df_outc = restore_metadata(schema_outc,df_outc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d9742ef-a8dd-4281-a8a1-91cc3537e4c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# salvar parquet final\n",
    "df_outc.write.mode('overwrite').parquet(\"dbfs:/FileStore/FAERS-grupo-4/outc_final\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Process_and_Save_Final_Outcome",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
