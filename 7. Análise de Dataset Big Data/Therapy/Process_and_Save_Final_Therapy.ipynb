{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "290351a8-f45d-410f-a79c-ea3668e9efe7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Notebook 3: Funções de Transformação e Limpeza de Dados\n",
    "\n",
    "## Introdução\n",
    "Neste notebook, iremos implementar funções para transformar e limpar os dados da tabela `therapy` do conjunto de dados FAERS. Com base na análise exploratória realizada no notebook anterior (Exploratory_Data_Analysis_Therapy), identificámos várias áreas que requerem atenção, incluindo a padronização de datas, tratamento de valores nulos e correção de inconsistências.\n",
    "\n",
    "## Objetivos\n",
    "- Criar funções reutilizáveis para a limpeza e transformação de dados.\n",
    "- Padronizar formatos de data.\n",
    "- Lidar com valores nulos e inconsistentes.\n",
    "- Preparar os dados para análises futuras.\n",
    "\n",
    "## Estrutura do Notebook\n",
    "1. **Importação de Bibliotecas**\n",
    "2. **Carregamento dos Dados**\n",
    "3. **Definição do Schema**\n",
    "4. **Definição de Metadados**\n",
    "5. **Funções de Transformação**\n",
    "   - Padronização de Datas\n",
    "   - Tratamento de Valores Nulos\n",
    "   - Correção de Inconsistências\n",
    "   - Conversão de Unidades de Duração\n",
    "6. **Aplicação das Funções**\n",
    "7. **Validação dos Dados Transformados**\n",
    "8. **Gravação dos Dados Limpos**\n",
    "\n",
    "Vamos começar por implementar cada uma destas secções para garantir uma limpeza e transformação eficaz dos nossos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "667c30a3-904e-4b78-93b0-6ce1c695d7d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import IntegerType, DateType\n",
    "from pyspark.sql.functions import to_date, col, when, expr\n",
    "from pyspark.sql.functions import col, when, to_date, expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2f35ec4-be5c-44f9-8958-142975c96f92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Carregar dados do Parquet\n",
    "df_therapy = spark.read.parquet('dbfs:/FileStore/FAERS-grupo-4/therapy_raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2da5106d-e320-463e-a14d-ce0d4c0d4dd9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Definir schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf9a31a6-e3a3-4e7f-b2bd-f32289852aec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType, DoubleType\n",
    "\n",
    "\n",
    "# Definir o esquema final manualmente\n",
    "therapy_schema = StructType([\n",
    "    StructField(\"primaryid\", IntegerType(), True),\n",
    "    StructField(\"caseid\", IntegerType(), True),\n",
    "    StructField(\"dsg_drug_seq\", IntegerType(), True),\n",
    "    StructField(\"start_dt\", StringType(), True),\n",
    "    StructField(\"end_dt\", StringType(), True),\n",
    "    StructField(\"dur\", IntegerType(), True),\n",
    "    StructField(\"dur_cod\", StringType(), True),\n",
    "    StructField(\"dur_days\", IntegerType(), True),\n",
    "    StructField(\"ongoing\", BooleanType(), True),\n",
    "    StructField(\"datas_consistentes\", BooleanType(), True),\n",
    "    StructField(\"duracao_em_dias\", DoubleType(), True),\n",
    "    StructField(\"terapia_em_andamento\", BooleanType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56a28074-fbe6-4a81-bebc-5595c3e6e867",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Definir metadados atualizados\n",
    "meta_data_therapy = {\n",
    "    \"primaryid\": \"Unique number for identifying a FAERS report. This is a concatenated key of Case ID and Case Version Number. It is the identifier for the case sequence version number as reported by the manufacturer.\",\n",
    "    \"caseid\": \"Number for identifying a FAERS case.\",\n",
    "    \"dsg_drug_seq\": \"Drug sequence number for identifying a drug for a Case.\",\n",
    "    \"start_dt\": \"Date the therapy was started (or re-started) for this drug (YYYYMMDD). If a complete date not available, a partial date is provided.\",\n",
    "    \"end_dt\": \"Date therapy was stopped for this drug.\",\n",
    "    \"dur\": \"Numeric value of the duration (length) of therapy.\",\n",
    "    \"dur_cod\": \"Unit abbreviation for duration of therapy.\",\n",
    "    \"dur_days\": \"Duration of therapy in days.\",\n",
    "    \"ongoing\": \"Boolean flag indicating if the therapy is ongoing.\",\n",
    "    \"datas_consistentes\": \"Boolean flag indicating if the therapy dates are consistent.\",\n",
    "    \"duracao_em_dias\": \"Duration of therapy in days calculated.\",\n",
    "    \"terapia_em_andamento\": \"Boolean flag indicating if the therapy is ongoing based on missing end date.\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71d858bd-249b-49dd-a8d9-a014c3ea496a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType, DoubleType\n",
    "\n",
    "# Definir o esquema final manualmente\n",
    "therapy_schema_final = StructType([\n",
    "    StructField(\"primaryid\", IntegerType(), True),\n",
    "    StructField(\"caseid\", IntegerType(), True),\n",
    "    StructField(\"dsg_drug_seq\", IntegerType(), True),\n",
    "    StructField(\"start_dt\", StringType(), True),\n",
    "    StructField(\"end_dt\", StringType(), True),\n",
    "    StructField(\"dur\", IntegerType(), True),\n",
    "    StructField(\"dur_cod\", StringType(), True),\n",
    "    StructField(\"dur_days\", IntegerType(), True),\n",
    "    StructField(\"ongoing\", BooleanType(), True),\n",
    "    StructField(\"datas_consistentes\", BooleanType(), True),\n",
    "    StructField(\"duracao_em_dias\", DoubleType(), True),\n",
    "    StructField(\"terapia_em_andamento\", BooleanType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d59e4b89-47af-4051-b638-b7e1a00354f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Renomear colunas para corresponder aos metadados\n",
    "df_therapy = df_therapy.withColumnRenamed(\"primaryid\", \"primaryid\") \\\n",
    "    .withColumnRenamed(\"caseid\", \"caseid\") \\\n",
    "    .withColumnRenamed(\"dsg_drug_seq\", \"dsg_drug_seq\") \\\n",
    "    .withColumnRenamed(\"start_dt\", \"start_dt\") \\\n",
    "    .withColumnRenamed(\"end_dt\", \"end_dt\") \\\n",
    "    .withColumnRenamed(\"dur\", \"dur\") \\\n",
    "    .withColumnRenamed(\"dur_cod\", \"dur_cod\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dc0d05a-24c7-41f5-90c4-aebe097bacd1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Configurar os metadados e renomear colunas da tabela"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "384edc19-d409-4c92-8b1f-cf00363b6b44",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Funções de Transformação, validação dados e para guardar parquet 'therapy_final' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb0d26ee-288c-4eff-9563-c2b936d4c3c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, datediff, to_date, lit, avg, expr, coalesce, last_day, concat, round, isnan, date_add, date_sub, stddev\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def tratar_dados_therapy(df):\n",
    "    # Converter datas, tratando datas parciais\n",
    "    df = df.withColumn(\"start_dt\", \n",
    "        when(col(\"start_dt\").cast(\"string\").rlike(\"^[0-9]{8}$\"), to_date(col(\"start_dt\").cast(\"string\"), \"yyyyMMdd\"))\n",
    "        .when(col(\"start_dt\").cast(\"string\").rlike(\"^[0-9]{6}$\"), to_date(concat(col(\"start_dt\").cast(\"string\"), lit(\"01\")), \"yyyyMMdd\"))\n",
    "        .when(col(\"start_dt\").cast(\"string\").rlike(\"^[0-9]{4}$\"), to_date(concat(col(\"start_dt\").cast(\"string\"), lit(\"0101\")), \"yyyyMMdd\"))\n",
    "        .otherwise(None)\n",
    "    )\n",
    "    df = df.withColumn(\"end_dt\", \n",
    "        when(col(\"end_dt\").cast(\"string\").rlike(\"^[0-9]{8}$\"), to_date(col(\"end_dt\").cast(\"string\"), \"yyyyMMdd\"))\n",
    "        .when(col(\"end_dt\").cast(\"string\").rlike(\"^[0-9]{6}$\"), last_day(to_date(concat(col(\"end_dt\").cast(\"string\"), lit(\"01\")), \"yyyyMMdd\")))\n",
    "        .when(col(\"end_dt\").cast(\"string\").rlike(\"^[0-9]{4}$\"), to_date(concat(col(\"end_dt\").cast(\"string\"), lit(\"1231\")), \"yyyyMMdd\"))\n",
    "        .otherwise(None)\n",
    "    )\n",
    "\n",
    "    # Normalizar duração para dias\n",
    "    df = df.withColumn(\"dur_days\", \n",
    "        when(col(\"dur\").isNotNull() & col(\"dur_cod\").isNotNull(),\n",
    "            round(when(col(\"dur_cod\") == \"YR\", col(\"dur\") * 365)\n",
    "            .when(col(\"dur_cod\") == \"MON\", col(\"dur\") * 30)\n",
    "            .when(col(\"dur_cod\") == \"WK\", col(\"dur\") * 7)\n",
    "            .when(col(\"dur_cod\") == \"DAY\", col(\"dur\"))\n",
    "            .when(col(\"dur_cod\") == \"HR\", col(\"dur\") / 24)\n",
    "            .when(col(\"dur_cod\") == \"MIN\", col(\"dur\") / (24 * 60))\n",
    "            .when(col(\"dur_cod\") == \"SEC\", col(\"dur\") / (24 * 60 * 60))\n",
    "            .otherwise(None)).cast(\"integer\")\n",
    "        ).otherwise(None)\n",
    "    )\n",
    "\n",
    "    # Calcular duração em dias inicial\n",
    "    df = df.withColumn(\"duracao_em_dias\", \n",
    "        when(col(\"start_dt\").isNotNull() & col(\"end_dt\").isNotNull() & (col(\"end_dt\") >= col(\"start_dt\")),\n",
    "             datediff(col(\"end_dt\"), col(\"start_dt\")) + 1)\n",
    "        .when(col(\"dur_days\").isNotNull(), col(\"dur_days\"))\n",
    "        .otherwise(None)\n",
    "    )\n",
    "\n",
    "    # Calcular estatísticas para identificar outliers\n",
    "    stats = df.select(avg(\"duracao_em_dias\").alias(\"mean\"), \n",
    "                      stddev(\"duracao_em_dias\").alias(\"std\")).first()\n",
    "    mean_duration = stats[\"mean\"]\n",
    "    std_duration = stats[\"std\"]\n",
    "    lower_bound = mean_duration - 3 * std_duration\n",
    "    upper_bound = mean_duration + 3 * std_duration\n",
    "\n",
    "    # Remover outliers e tratar durações negativas\n",
    "    df = df.withColumn(\"duracao_em_dias\", \n",
    "        when((col(\"duracao_em_dias\") < lower_bound) | (col(\"duracao_em_dias\") > upper_bound) | (col(\"duracao_em_dias\") < 0), None)\n",
    "        .otherwise(col(\"duracao_em_dias\"))\n",
    "    )\n",
    "\n",
    "    # Imputar média para casos sem informação suficiente\n",
    "    mean_duration_valid = df.filter(col(\"duracao_em_dias\").isNotNull()).select(avg(\"duracao_em_dias\")).first()[0]\n",
    "    df = df.withColumn(\"duracao_em_dias\", \n",
    "        when(col(\"duracao_em_dias\").isNull(), round(lit(mean_duration_valid)).cast(\"integer\"))\n",
    "        .otherwise(col(\"duracao_em_dias\").cast(\"integer\"))\n",
    "    )\n",
    "\n",
    "    # Recalcular datas com base na duração imputada\n",
    "    df = df.withColumn(\"end_dt\",\n",
    "        when(col(\"start_dt\").isNotNull() & col(\"end_dt\").isNull(),\n",
    "             date_add(col(\"start_dt\"), col(\"duracao_em_dias\") - 1))\n",
    "        .otherwise(col(\"end_dt\"))\n",
    "    )\n",
    "    df = df.withColumn(\"start_dt\",\n",
    "        when(col(\"start_dt\").isNull() & col(\"end_dt\").isNotNull(),\n",
    "             date_sub(col(\"end_dt\"), col(\"duracao_em_dias\") - 1))\n",
    "        .otherwise(col(\"start_dt\"))\n",
    "    )\n",
    "\n",
    "    # Corrigir casos onde end_dt é menor que start_dt\n",
    "    df = df.withColumn(\"end_dt\",\n",
    "        when((col(\"end_dt\").isNotNull()) & (col(\"start_dt\").isNotNull()) & (col(\"end_dt\") < col(\"start_dt\")),\n",
    "             date_add(col(\"start_dt\"), col(\"duracao_em_dias\") - 1))\n",
    "        .otherwise(col(\"end_dt\"))\n",
    "    )\n",
    "\n",
    "    # Identificar terapias em andamento\n",
    "    df = df.withColumn(\"terapia_em_andamento\", \n",
    "        when(col(\"end_dt\").isNull() & col(\"start_dt\").isNotNull(), True)\n",
    "        .otherwise(False)\n",
    "    )\n",
    "\n",
    "    # Verificar consistência das datas\n",
    "    df = df.withColumn(\"datas_consistentes\",\n",
    "        when(col(\"end_dt\").isNotNull() & col(\"start_dt\").isNotNull() & (col(\"end_dt\") >= col(\"start_dt\")), True)\n",
    "        .otherwise(False)\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "# Aplicar o tratamento\n",
    "df_therapy_tratado = tratar_dados_therapy(df_therapy)\n",
    "\n",
    "# Validar os dados\n",
    "def validar_dados(df):\n",
    "    print(\"Esquema do DataFrame:\")\n",
    "    df.printSchema()\n",
    "    \n",
    "    print(\"\\nResumo estatístico da duração em dias:\")\n",
    "    df.select(\"duracao_em_dias\").summary().show()\n",
    "    \n",
    "    print(\"\\nContagem de valores nulos:\")\n",
    "    for column in df.columns:\n",
    "        null_count = df.filter(col(column).isNull()).count()\n",
    "        print(f\"{column}: {null_count}\")\n",
    "    \n",
    "    print(\"\\nExemplos de registros:\")\n",
    "    df.show(5, truncate=False)\n",
    "\n",
    "    print(\"\\nDistribuição de terapias em andamento:\")\n",
    "    df.groupBy(\"terapia_em_andamento\").count().show()\n",
    "\n",
    "    print(\"\\nDistribuição de consistência de datas:\")\n",
    "    df.groupBy(\"datas_consistentes\").count().show()\n",
    "\n",
    "#validar_dados(df_therapy_tratado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc8b1fc3-fe09-4954-8e5f-ccc052e8ba99",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Função para restaurar metadados\n",
    "def restore_metadata(schema, df):\n",
    "    for field in schema.fields:\n",
    "        if field.name not in df.columns:\n",
    "            df = df.withColumn(field.name, lit(None).cast(field.dataType))\n",
    "        df = df.withColumnRenamed(field.name, field.name)\n",
    "    return df\n",
    "\n",
    "# Restaurar metadados após o processamento\n",
    "df_therapy_tratado = restore_metadata(therapy_schema_final, df_therapy_tratado)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0ca0b65-4f0b-4c4f-809f-b8a9e2d94491",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def validar_dados(df):\n",
    "    print(\"Esquema do DataFrame:\")\n",
    "    df.printSchema()\n",
    "    \n",
    "    print(\"\\nResumo estatístico da duração em dias:\")\n",
    "    df.select(\"duracao_em_dias\").summary().show()\n",
    "    \n",
    "    print(\"\\nContagem de valores nulos:\")\n",
    "    for column in df.columns:\n",
    "        null_count = df.filter(col(column).isNull()).count()\n",
    "        print(f\"{column}: {null_count}\")\n",
    "    \n",
    "    print(\"\\nExemplos de registros:\")\n",
    "    df.show(5, truncate=False)\n",
    "\n",
    "    print(\"\\nDistribuição de terapias em andamento:\")\n",
    "    df.groupBy(\"terapia_em_andamento\").count().show()\n",
    "\n",
    "    print(\"\\nDistribuição de consistência de datas:\")\n",
    "    df.groupBy(\"datas_consistentes\").count().show()\n",
    "\n",
    "#validar_dados(df_therapy_tratado)\n",
    "\n",
    "# Função para gravar dados\n",
    "def gravar_dados(df, caminho):\n",
    "    df.write.mode(\"overwrite\").parquet(caminho)\n",
    "\n",
    "# Especificar o caminho para salvar o DataFrame transformado\n",
    "save_path = \"dbfs:/FileStore/FAERS-grupo-4/therapy_final\"\n",
    "\n",
    "# Gravar o DataFrame transformado\n",
    "gravar_dados(df_therapy_tratado, save_path)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Process_and_Save_Final_Therapy",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
